{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677bfb6e",
   "metadata": {},
   "source": [
    "ü™ôüéØ PLAN ADAPT√â √Ä TON SUJET : COMMODITIES & FACTORS\n",
    "0. Executive Summary\n",
    "‚Ä¢\tBr√®ve pr√©sentation des objectifs, m√©thodo, r√©sultats majeurs et recommandations (1 page max).\n",
    "\n",
    "Annexes\n",
    "‚Ä¢\tTables de performances d√©taill√©es, graphiques, codes, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb9cfc",
   "metadata": {},
   "source": [
    "# Volatility Targeting Strategies for Commodities and Factors\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements and analyzes volatility targeting strategies across two asset classes:\n",
    "1. Traditional investment factors (Market, Size, Value, Momentum)\n",
    "2. Commodity markets (Gold, Oil, Copper, etc.)\n",
    "\n",
    "Our goal is to determine whether the volatility targeting approach documented by Moreira & Muir (2017) in equity markets extends effectively to commodity markets, which exhibit different volatility patterns and economic drivers.\n",
    "\n",
    "### Key Research Questions\n",
    "\n",
    "1. Does volatility targeting improve risk-adjusted performance in commodity markets?\n",
    "2. Which volatility estimation methods produce the most effective scaling signals?\n",
    "3. How do transaction costs and leverage constraints affect strategy implementation?\n",
    "4. Do volatility-managed commodities provide different benefits across economic cycles?\n",
    "\n",
    "### Methodology Summary\n",
    "\n",
    "1. Establish baseline performance for unmanaged factor and commodity portfolios\n",
    "2. Implement volatility forecasting models (Realized, EWMA, GARCH)\n",
    "3. Apply the Moreira & Muir scaling framework to both asset classes\n",
    "4. Analyze performance metrics across different market regimes\n",
    "5. Test robustness to implementation constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26b48b",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "‚Ä¢\tPourquoi les commodities sont pertinentes (diversification, inflation hedge, r√¥le macro‚Ä¶).\n",
    "‚Ä¢\tPourquoi tester la strat√©gie sur √† la fois des facteurs (benchmark acad√©mique) et des commodities (actifs concrets).\n",
    "‚Ä¢\tDouble objectif :\n",
    "o\tValider la strat√©gie sur des facteurs (r√©f√©rence th√©orique).\n",
    "o\tTester sa pertinence sur un univers plus op√©rationnel : les mati√®res premi√®res."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e84994",
   "metadata": {},
   "source": [
    "2. Th√©orie des strat√©gies volatility targeting\n",
    "‚Ä¢\tRappel du mod√®le de Moreira & Muir (scaling des rendements).\n",
    "‚Ä¢\tCas sp√©cifiques aux commodities : volatilit√© plus √©lev√©e, effets saisonniers, impact des roll-overs.\n",
    "‚Ä¢\tLimites potentielles : absence de facteur ‚Äúfondamental‚Äù clair comme pour les actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced142fc",
   "metadata": {},
   "source": [
    "## 1. Moreira & Muir Model for Return Scaling\n",
    "\n",
    "### 1.1 Theoretical Framework\n",
    "\n",
    "1. **Core Premise:**\n",
    "   1. Volatility is persistent and predictable\n",
    "   2. Returns have little-to-no relationship with future volatility\n",
    "   3. Risk-adjusted returns are higher in low-volatility periods\n",
    "\n",
    "2. **Mathematical Foundation:**\n",
    "   1. Standard portfolio theory suggests inverse relationship between optimal allocation and variance:\n",
    "      - $w_t = \\frac{\\mu_t}{\\gamma \\sigma_t^2}$ where:\n",
    "      - $w_t$ = portfolio weight at time t\n",
    "      - $\\mu_t$ = expected return\n",
    "      - $\\sigma_t^2$ = expected variance\n",
    "      - $\\gamma$ = risk aversion parameter\n",
    "   2. Moreira & Muir's key insight: When $\\mu_t$ is relatively constant, optimal allocation should be inversely proportional to variance\n",
    "\n",
    "3. **Implementation Framework:**\n",
    "   1. Scale exposure to risky assets inversely with predicted volatility\n",
    "   2. Target constant level of volatility over time\n",
    "   3. Mathematical expression: $r_{t+1}^{scaled} = \\frac{\\sigma_{target}}{\\hat{\\sigma}_t} \\times r_{t+1}$\n",
    "\n",
    "4. **Key Academic Sources:**\n",
    "   1. Moreira, A., & Muir, T. (2017). Volatility-Managed Portfolios. Journal of Finance, 72(4), 1611-1644\n",
    "   2. Fleming, J., Kirby, C., & Ostdiek, B. (2001). The Economic Value of Volatility Timing. Journal of Finance, 56(1), 329-352\n",
    "   3. Harvey, C. R., Hoyle, E., Korgaonkar, R., Rattray, S., Sargaison, M., & Van Hemert, O. (2018). The Impact of Volatility Targeting. Journal of Portfolio Management, 45(1), 14-33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb2014",
   "metadata": {},
   "source": [
    "### 1.2 Practical Implementation Steps\n",
    "\n",
    "1. **Volatility Target Selection:**\n",
    "   1. Define target annualized volatility (e.g., 10%, 15%, 20%)\n",
    "   2. Consider risk tolerance, regulatory constraints, and asset class characteristics\n",
    "   3. For commodities: typically higher targets (15-20%) may be appropriate due to higher native volatility\n",
    "\n",
    "2. **Volatility Estimation Window:**\n",
    "   1. Short-term reactivity vs. long-term stability trade-off\n",
    "   2. Common choices: 1-month (21 days), 3-months (63 days), or 6-months (126 days)\n",
    "   3. Commodity-specific consideration: account for potential seasonality effects\n",
    "\n",
    "3. **Position Sizing Algorithm:**\n",
    "   1. Calculate scaling factor: $s_t = \\frac{\\sigma_{target}}{\\hat{\\sigma}_t}$\n",
    "   2. Apply scaling to position: $Position_t = s_t \\times BasePosition_t$\n",
    "   3. Implement maximum leverage constraints (typically 2x-3x)\n",
    "\n",
    "4. **Rebalancing Frequency:**\n",
    "   1. Daily, weekly, or monthly rebalancing based on new volatility estimates\n",
    "   2. Consider trading costs vs. tracking error to target volatility\n",
    "   3. Potential use of rebalancing bands to reduce turnover\n",
    "\n",
    "5. **Potential Implementation Challenges:**\n",
    "   1. Look-ahead bias risk in backtesting (ensure proper time-indexing)\n",
    "   2. Transaction costs from frequent rebalancing\n",
    "   3. Cash management during high volatility periods (when scaling < 1)\n",
    "   4. Futures-specific issues for commodities (margin requirements, roll costs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc318a",
   "metadata": {},
   "source": [
    "## 2. Volatility Prediction Methods\n",
    "\n",
    "### 2.1 Theoretical Overview of Volatility Estimation\n",
    "\n",
    "1. **Realized Volatility:**\n",
    "   1. Definition: Historical standard deviation of returns over a fixed lookback window\n",
    "   2. Mathematical expression: $\\sigma_t^{realized} = \\sqrt{\\frac{1}{n-1} \\sum_{i=t-n}^{t-1} (r_i - \\bar{r})^2}$\n",
    "   3. Key parameters: Window length (n), sampling frequency\n",
    "   4. Strengths: Simple to implement, model-free, transparent\n",
    "   5. Weaknesses: Equal weighting of all observations, slow to adapt to volatility regime changes\n",
    "\n",
    "2. **Exponentially Weighted Moving Average (EWMA):**\n",
    "   1. Definition: Weighted average of squared returns with exponentially decaying weights\n",
    "   2. Mathematical expression: $\\sigma_t^2 = (1-\\lambda) r_{t-1}^2 + \\lambda \\sigma_{t-1}^2$\n",
    "   3. Key parameter: Decay factor Œª (typically 0.94 for daily data, per RiskMetrics)\n",
    "   4. Strengths: More responsive to recent market changes, computationally efficient\n",
    "   5. Weaknesses: Selection of optimal Œª, no mean-reversion component\n",
    "\n",
    "3. **GARCH Models:**\n",
    "   1. Definition: Generalized Autoregressive Conditional Heteroskedasticity - models variance as function of past returns and past variance\n",
    "   2. Mathematical expression (GARCH(1,1)): $\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta\\sigma_{t-1}^2$\n",
    "   3. Key parameters: œâ (long-term variance), Œ± (return shock impact), Œ≤ (volatility persistence)\n",
    "   4. Strengths: Mean-reverting, captures volatility clustering, models persistence\n",
    "   5. Weaknesses: Parameter estimation complexity, assumption of normally distributed returns\n",
    "\n",
    "4. **Academic Sources:**\n",
    "   1. Hansen, P. R., & Lunde, A. (2005). A forecast comparison of volatility models: Does anything beat a GARCH(1,1)?. Journal of Applied Econometrics, 20(7), 873-889\n",
    "   2. Andersen, T. G., Bollerslev, T., Diebold, F. X., & Labys, P. (2003). Modeling and forecasting realized volatility. Econometrica, 71(2), 579-625\n",
    "   3. Poon, S. H., & Granger, C. W. (2003). Forecasting volatility in financial markets: A review. Journal of Economic Literature, 41(2), 478-539"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ec42c",
   "metadata": {},
   "source": [
    "### 2.2 Implementation Methodology\n",
    "\n",
    "1. **Data Preparation for Volatility Estimation:**\n",
    "   1. Convert price series to return series (log or simple returns)\n",
    "   2. Screen for outliers that might distort volatility estimates\n",
    "   3. Handle missing data points (interpolation or omission)\n",
    "   4. Consider the appropriate return frequency (daily vs. intraday)\n",
    "\n",
    "2. **Realized Volatility Implementation Steps:**\n",
    "   1. Define the lookback window (e.g., 21, 63, or 126 days)\n",
    "   2. Calculate rolling standard deviation of returns\n",
    "   3. Annualize the estimate by multiplying by $\\sqrt{252}$ (for daily data)\n",
    "   4. Consider window length trade-offs: shorter windows are more reactive but noisier\n",
    "   5. Potential improvement: Use overlapping windows to increase estimation efficiency\n",
    "\n",
    "3. **EWMA Implementation Steps:**\n",
    "   1. Select decay parameter Œª (standard is 0.94, but can be optimized)\n",
    "   2. Initialize variance estimate (often with realized variance of first window)\n",
    "   3. Recursively update the estimate with new return observations\n",
    "   4. Consider adaptive Œª approaches for different market regimes\n",
    "   5. Evaluate half-life of information decay to ensure relevance\n",
    "\n",
    "4. **GARCH Implementation Steps:**\n",
    "   1. Select GARCH model specification (typically GARCH(1,1) is sufficient)\n",
    "   2. Estimate parameters using maximum likelihood (requires optimization)\n",
    "   3. Perform model diagnostics (standardized residuals, information criteria)\n",
    "   4. Generate out-of-sample forecasts for the next period\n",
    "   5. Consider extensions for asymmetry (EGARCH, GJR-GARCH) or long memory (FIGARCH)\n",
    "\n",
    "5. **Implementation Challenges Specific to Commodities:**\n",
    "   1. Seasonality effects requiring seasonal adjustment\n",
    "   2. Discontinuities around futures roll dates\n",
    "   3. Fat-tailed return distributions more common than in equities\n",
    "   4. Structural breaks due to supply/demand shocks\n",
    "   5. Market-specific factors (e.g., weather impacts for agricultural commodities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc189b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of key volatility estimation methods\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "\n",
    "def calculate_volatility_measures(returns, realized_window=21, ewma_lambda=0.94, garch_p=1, garch_q=1):\n",
    "    \"\"\"\n",
    "    Calculate different volatility estimates for a return series\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pandas.Series\n",
    "        Daily returns series\n",
    "    realized_window : int\n",
    "        Lookback window for realized volatility\n",
    "    ewma_lambda : float\n",
    "        Decay factor for EWMA volatility (between 0 and 1)\n",
    "    garch_p : int\n",
    "        GARCH lag order\n",
    "    garch_q : int\n",
    "        ARCH lag order\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with different volatility estimates\n",
    "    \"\"\"\n",
    "    # Calculate annualization factor\n",
    "    ann_factor = np.sqrt(252)  # Assuming daily returns\n",
    "    \n",
    "    # 1. Realized Volatility\n",
    "    realized_vol = returns.rolling(window=realized_window).std() * ann_factor\n",
    "    \n",
    "    # 2. EWMA Volatility\n",
    "    ewma_vol = pd.Series(index=returns.index)\n",
    "    ewma_vol.iloc[0] = returns.iloc[0] if not np.isnan(returns.iloc[0]) else 0\n",
    "    \n",
    "    # Initialize with realized vol of first window if possible\n",
    "    if len(returns) >= realized_window:\n",
    "        init_var = returns.iloc[:realized_window].var()\n",
    "    else:\n",
    "        init_var = returns.iloc[0]**2\n",
    "    \n",
    "    # Recursive calculation\n",
    "    var = init_var\n",
    "    for t in range(1, len(returns)):\n",
    "        if np.isnan(returns.iloc[t-1]):\n",
    "            var = var  # Keep previous variance if return is missing\n",
    "        else:\n",
    "            var = (1 - ewma_lambda) * returns.iloc[t-1]**2 + ewma_lambda * var\n",
    "        ewma_vol.iloc[t] = np.sqrt(var) * ann_factor\n",
    "    \n",
    "    # 3. GARCH Volatility (using arch_model from arch package)\n",
    "    # This requires a longer time series for reliable estimation\n",
    "    garch_vol = pd.Series(index=returns.index)\n",
    "    \n",
    "    # Only attempt GARCH if we have sufficient data\n",
    "    min_obs = 100  # Minimum observations for GARCH estimation\n",
    "    if len(returns.dropna()) > min_obs:\n",
    "        try:\n",
    "            # Fit GARCH model\n",
    "            model = arch_model(returns.dropna(), vol='Garch', p=garch_p, q=garch_q)\n",
    "            model_fit = model.fit(disp='off')\n",
    "            \n",
    "            # Extract conditional volatility\n",
    "            garch_conditional_vol = model_fit.conditional_volatility\n",
    "            \n",
    "            # Map back to original index\n",
    "            garch_vol = pd.Series(\n",
    "                garch_conditional_vol * ann_factor,\n",
    "                index=returns.dropna().index\n",
    "            ).reindex(returns.index)\n",
    "        except:\n",
    "            print(\"GARCH estimation failed. Using NaN values.\")\n",
    "    \n",
    "    # Combine all volatility measures\n",
    "    vol_df = pd.DataFrame({\n",
    "        'Realized_Vol': realized_vol,\n",
    "        'EWMA_Vol': ewma_vol,\n",
    "        'GARCH_Vol': garch_vol\n",
    "    })\n",
    "    \n",
    "    return vol_df\n",
    "\n",
    "# Function to plot volatility comparison\n",
    "def plot_volatility_comparison(vol_df, title=\"Volatility Estimation Comparison\", figsize=(12, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for col in vol_df.columns:\n",
    "        plt.plot(vol_df.index, vol_df[col], label=col)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Annualized Volatility')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Example function implementing the Moreira & Muir scaling approach\n",
    "def volatility_scaling(returns, vol_window=21, target_vol=0.15, max_leverage=3.0, ann_factor=252):\n",
    "    \"\"\"\n",
    "    Implement volatility targeting according to Moreira & Muir (2017)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns: pandas.Series\n",
    "        Daily returns of the asset\n",
    "    vol_window: int\n",
    "        Lookback window for volatility estimation (in trading days)\n",
    "    target_vol: float\n",
    "        Target annualized volatility (as decimal, e.g., 0.15 for 15%)\n",
    "    max_leverage: float\n",
    "        Maximum allowed leverage\n",
    "    ann_factor: int\n",
    "        Annualization factor (252 for daily returns)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (scaled_returns, scaling_factors)\n",
    "        scaled_returns: pandas.Series - volatility-scaled returns\n",
    "        scaling_factors: pandas.Series - leverage applied at each point\n",
    "    \"\"\"\n",
    "    # Calculate rolling volatility (standard deviation)\n",
    "    rolling_vol = returns.rolling(window=vol_window).std() * np.sqrt(ann_factor)\n",
    "    \n",
    "    # Calculate scaling factors (with max leverage constraint)\n",
    "    scaling_factors = target_vol / rolling_vol\n",
    "    scaling_factors = scaling_factors.clip(upper=max_leverage)\n",
    "    \n",
    "    # Align scaling factors with next-day returns (avoid look-ahead bias)\n",
    "    aligned_scaling = scaling_factors.shift(1)\n",
    "    \n",
    "    # Apply scaling to returns\n",
    "    scaled_returns = returns * aligned_scaling\n",
    "    \n",
    "    # Remove NaN values from the start of the series\n",
    "    valid_index = ~scaled_returns.isna()\n",
    "    scaled_returns = scaled_returns[valid_index]\n",
    "    scaling_used = aligned_scaling[valid_index]\n",
    "    \n",
    "    return scaled_returns, scaling_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61aad6",
   "metadata": {},
   "source": [
    "## 3. Rolling Strategy Construction\n",
    "\n",
    "### 3.1 Theoretical Framework for Rolling Implementation\n",
    "\n",
    "1. **Rolling Window Approach in Financial Modeling:**\n",
    "   1. Definition: A method that uses overlapping or non-overlapping windows of historical data to estimate parameters and make forward-looking decisions\n",
    "   2. Purpose in volatility targeting: Ensures realistic implementation by using only information available at each decision point\n",
    "   3. Common window parameters: Estimation window (lookback period) and holding period (rebalancing frequency)\n",
    "   4. Mathematical representation: For each time point t, use data from [t-n, t-1] to inform decision at time t\n",
    "\n",
    "2. **Out-of-Sample Implementation Requirements:**\n",
    "   1. Strict time-indexing to prevent look-ahead bias (information from time t cannot inform decisions at t)\n",
    "   2. Parameter re-estimation at each step rather than fixed parameters for the entire backtest\n",
    "   3. Walk-forward validation approach to simulate real-world implementation\n",
    "   4. Consideration of data availability constraints at each historical point\n",
    "\n",
    "3. **Rolling Strategy Calibration:**\n",
    "   1. Parameter stability assessment across different market regimes\n",
    "   2. Sensitivity analysis to window length selection\n",
    "   3. Regime-dependent parameter adaptation methods\n",
    "   4. Statistical tests for parameter significance and robustness over time\n",
    "\n",
    "4. **Academic Sources:**\n",
    "   1. Pesaran, M. H., & Timmermann, A. (2007). Selection of estimation window in the presence of breaks. Journal of Econometrics, 137(1), 134-161\n",
    "   2. Inoue, A., Jin, L., & Rossi, B. (2017). Rolling window selection for out-of-sample forecasting with time-varying parameters. Journal of Econometrics, 196(1), 55-67\n",
    "   3. Barroso, P., & Santa-Clara, P. (2015). Momentum has its moments. Journal of Financial Economics, 116(1), 111-120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf0d17",
   "metadata": {},
   "source": [
    "### 3.2 Implementation Methodology\n",
    "\n",
    "1. **Setting Up the Rolling Framework:**\n",
    "   1. Define estimation window length for volatility calculation (e.g., 63 days for 3-month window)\n",
    "   2. Define rebalancing frequency (e.g., daily, weekly, or monthly)\n",
    "   3. Establish starting point with sufficient historical data (minimum estimation window)\n",
    "   4. Create data structures to store results (scaled returns, positions, scaling factors)\n",
    "   5. Implement chronological iteration through time series to simulate real-world implementation\n",
    "\n",
    "2. **Volatility Forecast Step:**\n",
    "   1. For each time point t, use returns from [t-n, t-1] to estimate volatility\n",
    "   2. Apply your selected volatility models (realized, EWMA, GARCH)\n",
    "   3. Store volatility forecasts in forward-looking data structure\n",
    "   4. Compare volatility forecasts across models to assess stability\n",
    "   5. Consider conditional forecasts based on market regime indicators\n",
    "\n",
    "3. **Position Sizing Step:**\n",
    "   1. Calculate scaling factor for each asset as œÉ_target/œÉ_forecast\n",
    "   2. Apply maximum leverage constraints (typically 2x-3x)\n",
    "   3. Optionally implement minimum position sizes for practical considerations\n",
    "   4. Create position matrix through time for each asset and model\n",
    "   5. Calculate capital allocation required for implementation\n",
    "\n",
    "4. **Portfolio Rebalancing Step:**\n",
    "   1. Calculate target position changes based on new scaling factors\n",
    "   2. Estimate transaction costs from position changes\n",
    "   3. Implement rebalancing threshold rules to reduce turnover\n",
    "   4. Calculate net realized returns after accounting for costs\n",
    "   5. Update portfolio values and weights\n",
    "\n",
    "5. **Implementation Challenges and Solutions:**\n",
    "   1. Challenge: Non-synchronous trading hours between commodities and equity factors\n",
    "      - Solution: Align data using previous day closing prices for next day signals\n",
    "   2. Challenge: Data gaps in commodity futures during roll periods\n",
    "      - Solution: Implement roll-adjusted continuous price series\n",
    "   3. Challenge: Computational efficiency in daily rebalancing\n",
    "      - Solution: Vectorized operations and optimized calculation methods\n",
    "   4. Challenge: Parameter instability during extreme volatility\n",
    "      - Solution: Implement regime-switching models or shrinkage estimators\n",
    "   5. Challenge: Trading limitations during market stress\n",
    "      - Solution: Incorporate liquidity-based position caps during high volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of rolling strategy construction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def construct_rolling_strategy(returns_df, vol_estimation_functions, \n",
    "                               estimation_window=63, target_vol=0.15, \n",
    "                               max_leverage=3.0, rebalance_freq='D',\n",
    "                               transaction_cost_bps=10):\n",
    "    \"\"\"\n",
    "    Implements rolling volatility-targeting strategy across multiple assets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns_df : pandas.DataFrame\n",
    "        DataFrame with asset returns in columns\n",
    "    vol_estimation_functions : dict\n",
    "        Dictionary mapping vol model names to their functions\n",
    "    estimation_window : int\n",
    "        Lookback window for volatility estimation in days\n",
    "    target_vol : float\n",
    "        Target annualized volatility (decimal)\n",
    "    max_leverage : float\n",
    "        Maximum leverage constraint\n",
    "    rebalance_freq : str\n",
    "        Pandas frequency string ('D'=daily, 'W'=weekly, 'M'=monthly)\n",
    "    transaction_cost_bps : float\n",
    "        Transaction costs in basis points\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict of pandas.DataFrame\n",
    "        Dict containing scaled returns, scaling factors, and positions\n",
    "    \"\"\"\n",
    "    # Initialize results containers\n",
    "    assets = returns_df.columns\n",
    "    models = list(vol_estimation_functions.keys())\n",
    "    \n",
    "    # Create rebalancing dates (daily, weekly, monthly)\n",
    "    all_dates = returns_df.index\n",
    "    rebalance_dates = returns_df.asfreq(rebalance_freq).index\n",
    "    \n",
    "    # Store results in multi-level DataFrames\n",
    "    scaled_returns = {}\n",
    "    scaling_factors = {}\n",
    "    positions = {}\n",
    "    turnover = {}\n",
    "    \n",
    "    # Annualization factor\n",
    "    ann_factor = np.sqrt(252)  # Assuming daily returns\n",
    "    \n",
    "    # Loop through volatility models\n",
    "    for model_name, vol_func in vol_estimation_functions.items():\n",
    "        print(f\"Processing {model_name} model...\")\n",
    "        \n",
    "        # Initialize model-specific results\n",
    "        scaled_returns[model_name] = pd.DataFrame(index=all_dates, columns=assets)\n",
    "        scaling_factors[model_name] = pd.DataFrame(index=all_dates, columns=assets)\n",
    "        positions[model_name] = pd.DataFrame(index=all_dates, columns=assets)\n",
    "        turnover[model_name] = pd.DataFrame(index=all_dates, columns=assets)\n",
    "        \n",
    "        # Loop through assets\n",
    "        for asset in tqdm(assets, desc=f\"Assets for {model_name}\"):\n",
    "            asset_returns = returns_df[asset]\n",
    "            \n",
    "            # Initialize positions and previous positions\n",
    "            prev_position = 0\n",
    "            \n",
    "            # Implement rolling strategy\n",
    "            for t in range(estimation_window, len(all_dates)):\n",
    "                current_date = all_dates[t]\n",
    "                \n",
    "                # Only rebalance on rebalance dates\n",
    "                if current_date in rebalance_dates:\n",
    "                    # Get historical data for estimation window\n",
    "                    hist_returns = asset_returns.iloc[t-estimation_window:t]\n",
    "                    \n",
    "                    # Calculate volatility forecast using the specified model\n",
    "                    vol_forecast = vol_func(hist_returns)\n",
    "                    \n",
    "                    # Calculate scaling factor with leverage constraint\n",
    "                    scaling = min(target_vol / (vol_forecast * ann_factor), max_leverage)\n",
    "                    \n",
    "                    # Update position\n",
    "                    new_position = scaling\n",
    "                    \n",
    "                    # Calculate turnover\n",
    "                    trade = new_position - prev_position\n",
    "                    turnover[model_name].loc[current_date, asset] = abs(trade)\n",
    "                    \n",
    "                    # Apply transaction costs\n",
    "                    tc = abs(trade) * (transaction_cost_bps / 10000)  # Convert bps to decimal\n",
    "                    \n",
    "                    # Store results\n",
    "                    scaling_factors[model_name].loc[current_date, asset] = scaling\n",
    "                    positions[model_name].loc[current_date, asset] = new_position\n",
    "                    \n",
    "                    # Update previous position for next iteration\n",
    "                    prev_position = new_position\n",
    "                else:\n",
    "                    # Keep previous position on non-rebalancing days\n",
    "                    positions[model_name].loc[current_date, asset] = prev_position\n",
    "                    scaling_factors[model_name].loc[current_date, asset] = prev_position\n",
    "                    turnover[model_name].loc[current_date, asset] = 0\n",
    "                \n",
    "                # Calculate scaled return for the day\n",
    "                if t < len(all_dates) - 1:  # Ensure we're not at the last data point\n",
    "                    next_return = asset_returns.iloc[t+1]\n",
    "                    position = positions[model_name].loc[current_date, asset]\n",
    "                    \n",
    "                    # Apply scaling to return\n",
    "                    scaled_ret = next_return * position\n",
    "                    \n",
    "                    # Store scaled return (properly aligned to next day)\n",
    "                    next_date = all_dates[t+1]\n",
    "                    scaled_returns[model_name].loc[next_date, asset] = scaled_ret\n",
    "        \n",
    "        # Forward fill positions and scaling factors for continuity\n",
    "        positions[model_name] = positions[model_name].ffill()\n",
    "        scaling_factors[model_name] = scaling_factors[model_name].ffill()\n",
    "    \n",
    "    return {\n",
    "        \"scaled_returns\": scaled_returns,\n",
    "        \"scaling_factors\": scaling_factors,\n",
    "        \"positions\": positions,\n",
    "        \"turnover\": turnover\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# Define volatility estimation functions dictionary\n",
    "def realized_volatility(returns, window=21):\n",
    "    \"\"\"Simple realized volatility estimation\"\"\"\n",
    "    return returns.std()\n",
    "\n",
    "def ewma_volatility(returns, lambda_param=0.94):\n",
    "    \"\"\"EWMA volatility estimation\"\"\"\n",
    "    # Initialize with realized vol\n",
    "    var = returns.var()\n",
    "    # Recursively calculate EWMA\n",
    "    for ret in returns:\n",
    "        if not np.isnan(ret):\n",
    "            var = (1 - lambda_param) * ret**2 + lambda_param * var\n",
    "    return np.sqrt(var)\n",
    "\n",
    "# Define vol_estimation_functions dictionary\n",
    "vol_models = {\n",
    "    \"Realized_21d\": lambda x: realized_volatility(x, window=21),\n",
    "    \"Realized_63d\": lambda x: realized_volatility(x, window=63),\n",
    "    \"EWMA_0.94\": lambda x: ewma_volatility(x, lambda_param=0.94)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a4ca46",
   "metadata": {},
   "source": [
    " \n",
    "3. Donn√©es\n",
    "‚Ä¢\tCommodities :\n",
    "o\tS√©lection de 5 √† 10 actifs (Gold, Oil, Copper, Corn, Natural Gas, etc.).\n",
    "o\tSource : Bloomberg, Yahoo Finance, ou autres.\n",
    "o\tFr√©quence : journali√®re ou mensuelle selon la dispo.\n",
    "‚Ä¢\tFacteurs (Fama-French + Mom) :\n",
    "o\tDonn√©es europ√©ennes si possible, sinon US.\n",
    "o\tSource : site de Kenneth French.\n",
    "‚Ä¢\tNettoyage, synchronisation, traitement des jours f√©ri√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88867d22",
   "metadata": {},
   "source": [
    "4. Construction des strat√©gies\n",
    "‚Ä¢\tFormule de scaling avec œÉ_target / œÉ_pred.\n",
    "‚Ä¢\tVolatilit√© pr√©dite via 3 approches :\n",
    "o\tR√©alis√©e 1 mois\n",
    "o\tR√©alis√©e 6 mois\n",
    "o\tEWMA ou GARCH (ou une approche machine learning simple si tu veux te d√©marquer).\n",
    "‚Ä¢\tMise en ≈ìuvre rolling pour √©viter look-ahead bias.\n",
    "‚Ä¢\tComparaison entre strat√©gie na√Øve et strat√©gie scal√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51904651",
   "metadata": {},
   "source": [
    " \n",
    "5. Analyse des performances\n",
    "‚Ä¢\tPar univers : facteurs vs commodities.\n",
    "‚Ä¢\tStatistiques : rendement, volatilit√©, Sharpe, max drawdown, skewness, kurtosis, expected shortfall, turnover.\n",
    "‚Ä¢\tR√©gressions alpha & appraisal ratio (si tu as un benchmark).\n",
    "‚Ä¢\tR√©sultats in-sample vs out-of-sample.\n",
    "‚Ä¢\tFocus sur les p√©riodes de crise (2008, 2020, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4cffa",
   "metadata": {},
   "source": [
    "6. Analyse Cycle √âconomique (optionnelle pour commodities mais diff√©renciante)\n",
    "‚Ä¢\tUtilisation des dates NBER.\n",
    "‚Ä¢\tPerformances en expansion vs r√©cession.\n",
    "‚Ä¢\tPeut r√©v√©ler des comportements d√©fensifs / cycliques des commodities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02683fc2",
   "metadata": {},
   "source": [
    "7. Contraintes r√©elles : transaction costs et leverage\n",
    "‚Ä¢\tCommodities : effets sp√©cifiques (roll-over, co√ªts implicites).\n",
    "‚Ä¢\tFacteurs : turnover √©lev√© sur momentum par exemple.\n",
    "‚Ä¢\tSc√©narios avec 10, 30, 50 bps.\n",
    "‚Ä¢\tPropositions :\n",
    "o\tSeuils de rebalancement.\n",
    "o\tInt√©gration des co√ªts dans la fonction d‚Äôoptimisation.\n",
    "‚Ä¢\t√âtude de sensibilit√© aux contraintes de levier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b67d38",
   "metadata": {},
   "source": [
    "8. Synth√®se et recommandations\n",
    "‚Ä¢\tR√©sum√© des r√©sultats cl√©s sur les deux univers.\n",
    "‚Ä¢\tInt√©r√™t concret du volatility targeting sur des actifs r√©els.\n",
    "‚Ä¢\tLimites pratiques (data quality, impl√©mentation).\n",
    "‚Ä¢\tRecommandations pour une mise en ≈ìuvre future.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29def6fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9003c3",
   "metadata": {},
   "source": [
    "## 4. Comparative Performance Analysis\n",
    "\n",
    "### 4.1 Theoretical Framework for Performance Evaluation\n",
    "\n",
    "1. **Performance Measurement in Risk-Adjusted Context:**\n",
    "   1. Definition: The assessment of investment returns in relation to the risk taken to achieve those returns\n",
    "   2. Core principle: Higher returns alone are insufficient; they must be evaluated per unit of risk\n",
    "   3. Two primary dimensions: absolute performance and risk-adjusted metrics\n",
    "   4. Mathematical foundation: Mean-variance framework and its extensions\n",
    "\n",
    "2. **Key Performance Metrics:**\n",
    "   1. **Return Metrics:**\n",
    "      1. Annualized return: $R_{ann} = (1 + R_{p})^{\\frac{252}{T}} - 1$ (for daily returns)\n",
    "      2. Cumulative return: $R_{cum} = \\prod_{t=1}^{T} (1 + r_t) - 1$\n",
    "   2. **Risk Metrics:**\n",
    "      1. Volatility: $\\sigma = \\sqrt{\\frac{252}{T-1}\\sum_{t=1}^{T}(r_t - \\bar{r})^2}$\n",
    "      2. Maximum drawdown: $MDD = \\max_{\\tau \\in (0,t)} [\\frac{V(\\tau) - V(t)}{V(\\tau)}]$\n",
    "      3. Value-at-Risk (VaR): $P(r_t < VaR_\\alpha) = \\alpha$\n",
    "      4. Expected Shortfall: $ES_\\alpha = E[r_t | r_t < VaR_\\alpha]$\n",
    "   3. **Risk-Adjusted Metrics:**\n",
    "      1. Sharpe ratio: $SR = \\frac{R_p - R_f}{\\sigma_p}$\n",
    "      2. Sortino ratio: $SOR = \\frac{R_p - R_f}{\\sigma_{down}}$\n",
    "      3. Calmar ratio: $CR = \\frac{R_{ann}}{MDD}$\n",
    "   4. **Trading Implementation Metrics:**\n",
    "      1. Turnover: $TO = \\frac{1}{T}\\sum_{t=1}^T \\sum_{i=1}^N |w_{i,t} - w_{i,t-1}|$\n",
    "      2. Average leverage: $Lev_{avg} = \\frac{1}{T}\\sum_{t=1}^T \\sum_{i=1}^N |w_{i,t}|$\n",
    "      3. Transaction cost impact: $TC = \\sum_{t=1}^T \\sum_{i=1}^N |w_{i,t} - w_{i,t-1}| \\cdot c$\n",
    "\n",
    "3. **Statistical Significance Tests:**\n",
    "   1. Jobson-Korkie test for Sharpe ratio differences\n",
    "   2. Bootstrap methods for robust confidence intervals\n",
    "   3. Ledoit-Wolf shrinkage for improved covariance estimation\n",
    "   4. Multiple hypothesis testing correction (e.g., Bonferroni, False Discovery Rate)\n",
    "\n",
    "4. **Academic Sources:**\n",
    "   1. Jobson, J.D., & Korkie, B.M. (1981). Performance hypothesis testing with the Sharpe and Treynor measures. Journal of Finance, 36(4), 889-908\n",
    "   2. Ledoit, O., & Wolf, M. (2008). Robust performance hypothesis testing with the Sharpe ratio. Journal of Empirical Finance, 15(5), 850-859\n",
    "   3. Harvey, C.R., & Liu, Y. (2015). Backtesting. Journal of Portfolio Management, 42(1), 13-28\n",
    "   4. Moreira, A., & Muir, T. (2019). Should long-term investors time volatility? Journal of Financial Economics, 131(3), 507-527"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a7ded",
   "metadata": {},
   "source": [
    "### 4.2 Implementation Methodology\n",
    "\n",
    "1. **Performance Metric Calculation Framework:**\n",
    "   1. Define standard evaluation periods (full sample, sub-periods, crisis vs. normal)\n",
    "   2. Implement consistent periodicity for comparison (daily, monthly, annualized)\n",
    "   3. Develop a comprehensive performance dashboard for strategy comparison\n",
    "   4. Establish benchmarks for relative performance assessment\n",
    "   5. Document all assumptions and methodological choices for transparency\n",
    "\n",
    "2. **Factor vs. Commodity Strategy Analysis Steps:**\n",
    "   1. Compare baseline (unmanaged) performance across asset classes\n",
    "   2. Evaluate incremental benefit of volatility targeting within each asset class\n",
    "   3. Assess volatility reduction efficiency across different instruments\n",
    "   4. Examine tail risk mitigation effects during market stress periods\n",
    "   5. Analyze correlation dynamics between managed and unmanaged strategies\n",
    "\n",
    "3. **Volatility Model Comparison Methodology:**\n",
    "   1. Calculate performance metrics for each volatility estimation approach\n",
    "   2. Implement statistical tests for significant differences between models\n",
    "   3. Evaluate model-specific performance across different market regimes\n",
    "   4. Analyze trade-off between model complexity and performance improvement\n",
    "   5. Assess parameter stability and robustness across asset classes\n",
    "\n",
    "4. **Transaction Cost Analysis Framework:**\n",
    "   1. Implement realistic transaction cost models (fixed, proportional, market impact)\n",
    "   2. Compare turnover profiles across volatility estimation methods\n",
    "   3. Develop cost-adjusted performance metrics for all strategies\n",
    "   4. Test sensitivity to cost assumptions (10, 30, 50 bps scenarios)\n",
    "   5. Evaluate implementation efficiency measures (cost per unit of risk reduction)\n",
    "\n",
    "5. **Implementation Challenges and Solutions:**\n",
    "   1. Challenge: Multiple comparison bias in strategy evaluation\n",
    "      - Solution: Apply multiple hypothesis testing corrections\n",
    "   2. Challenge: Parameter sensitivity affecting robust conclusions\n",
    "      - Solution: Implement parameter sensitivity analysis and cross-validation\n",
    "   3. Challenge: Outlier returns distorting performance metrics\n",
    "      - Solution: Use robust statistical methods and winsorization\n",
    "   4. Challenge: Strategy-specific data snooping\n",
    "      - Solution: Implement out-of-sample testing and walk-forward validation\n",
    "   5. Challenge: Time-varying performance dynamics\n",
    "      - Solution: Analyze rolling-window performance metrics and regime-dependent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of performance analysis functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_performance_metrics(returns, risk_free=0.0, periods_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics for a returns series\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pandas.Series or pandas.DataFrame\n",
    "        Series or DataFrame of returns\n",
    "    risk_free : float or pandas.Series\n",
    "        Risk-free rate (constant or time series)\n",
    "    periods_per_year : int\n",
    "        Number of periods in a year (252 for daily, 12 for monthly)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with performance metrics for each series\n",
    "    \"\"\"\n",
    "    # Convert Series to DataFrame if needed\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = pd.DataFrame(returns)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    metrics = {}\n",
    "    \n",
    "    for col in returns.columns:\n",
    "        series = returns[col].dropna()\n",
    "        if len(series) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate return metrics\n",
    "        cum_return = (1 + series).prod() - 1\n",
    "        ann_return = (1 + cum_return) ** (periods_per_year / len(series)) - 1\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        volatility = series.std() * np.sqrt(periods_per_year)\n",
    "        neg_returns = series[series < 0]\n",
    "        downside_vol = neg_returns.std() * np.sqrt(periods_per_year) if len(neg_returns) > 0 else 0\n",
    "        \n",
    "        # Calculate drawdowns\n",
    "        wealth_index = (1 + series).cumprod()\n",
    "        previous_peaks = wealth_index.cummax()\n",
    "        drawdowns = (wealth_index - previous_peaks) / previous_peaks\n",
    "        max_drawdown = drawdowns.min()\n",
    "        \n",
    "        # Calculate risk-adjusted metrics\n",
    "        sharpe = (ann_return - risk_free) / volatility if volatility != 0 else np.nan\n",
    "        sortino = (ann_return - risk_free) / downside_vol if downside_vol != 0 else np.nan\n",
    "        calmar = ann_return / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        skewness = series.skew()\n",
    "        kurtosis = series.kurtosis()  # Excess kurtosis\n",
    "        \n",
    "        # Calculate VaR and Expected Shortfall (CVaR)\n",
    "        var_95 = np.percentile(series, 5)  # 95% VaR (5th percentile of returns)\n",
    "        es_95 = series[series <= var_95].mean() if len(series[series <= var_95]) > 0 else var_95\n",
    "        \n",
    "        # Store all metrics\n",
    "        metrics[col] = {\n",
    "            'Cumulative Return': cum_return,\n",
    "            'Annualized Return': ann_return,\n",
    "            'Annualized Volatility': volatility,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Sortino Ratio': sortino,\n",
    "            'Max Drawdown': max_drawdown,\n",
    "            'Calmar Ratio': calmar,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'VaR (95%)': var_95,\n",
    "            'Expected Shortfall (95%)': es_95,\n",
    "            'Positive Months (%)': len(series[series > 0]) / len(series) * 100,\n",
    "            'Observation Count': len(series)\n",
    "        }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97808bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_strategy_comparison(returns_dict, figsize=(12, 8), title=\"Strategy Comparison\"):\n",
    "    \"\"\"\n",
    "    Plot cumulative returns and drawdowns for multiple strategies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns_dict : dict\n",
    "        Dictionary mapping strategy names to return Series\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "    title : str\n",
    "        Plot title\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Figure object with plots\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize, gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Plot cumulative returns\n",
    "    for name, returns in returns_dict.items():\n",
    "        cum_returns = (1 + returns).cumprod()\n",
    "        axes[0].plot(cum_returns.index, cum_returns, label=name)\n",
    "    \n",
    "    axes[0].set_title(f\"{title} - Cumulative Returns\")\n",
    "    axes[0].set_ylabel(\"Cumulative Return\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot drawdowns\n",
    "    for name, returns in returns_dict.items():\n",
    "        wealth_index = (1 + returns).cumprod()\n",
    "        previous_peaks = wealth_index.cummax()\n",
    "        drawdowns = (wealth_index - previous_peaks) / previous_peaks\n",
    "        axes[1].plot(drawdowns.index, drawdowns, label=name)\n",
    "    \n",
    "    axes[1].set_title(\"Drawdowns\")\n",
    "    axes[1].set_ylabel(\"Drawdown\")\n",
    "    axes[1].set_ylim(bottom=-1, top=0.05)  # Adjust as needed\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_market_regimes(returns, regime_data, regime_column='Regime'):\n",
    "    \"\"\"\n",
    "    Analyze performance across different market regimes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pandas.DataFrame\n",
    "        DataFrame with strategy returns as columns\n",
    "    regime_data : pandas.DataFrame\n",
    "        DataFrame with regime classification\n",
    "    regime_column : str\n",
    "        Column name in regime_data that contains regime information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict of pandas.DataFrame\n",
    "        Dict mapping regime names to performance metrics\n",
    "    \"\"\"\n",
    "    # Merge returns with regime data\n",
    "    merged_data = returns.join(regime_data[regime_column], how='inner')\n",
    "    \n",
    "    # Get unique regimes\n",
    "    regimes = merged_data[regime_column].unique()\n",
    "    \n",
    "    # Calculate performance metrics for each regime\n",
    "    regime_performance = {}\n",
    "    \n",
    "    for regime in regimes:\n",
    "        # Filter returns for this regime\n",
    "        regime_returns = merged_data[merged_data[regime_column] == regime].drop(columns=[regime_column])\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        metrics = calculate_performance_metrics(regime_returns)\n",
    "        \n",
    "        regime_performance[regime] = metrics\n",
    "    \n",
    "    return regime_performance\n",
    "\n",
    "# Example usage (would be implemented with real data)\n",
    "# performance_metrics = calculate_performance_metrics(all_strategy_returns)\n",
    "# fig = plot_strategy_comparison({'Unmanaged': unmanaged_returns, 'Vol-Managed': vol_managed_returns})\n",
    "# regime_performance = analyze_market_regimes(all_returns, economic_regimes, regime_column='NBER_Recession')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9acc5",
   "metadata": {},
   "source": [
    "## 5. Economic Cycle Analysis\n",
    "\n",
    "### 5.1 Theoretical Framework for Economic Regime Analysis\n",
    "\n",
    "1. **Economic Cycles and Asset Returns Relationship:**\n",
    "   1. Definition: The systematic variation in asset returns across different phases of the business cycle\n",
    "   2. Core premise: Different asset classes exhibit varying sensitivities to economic growth and inflation\n",
    "   3. Traditional factor model: Returns are decomposed into exposures to economic states\n",
    "   4. Mathematical representation: $R_{i,t} = \\alpha_i + \\sum_{j=1}^{K} \\beta_{i,j} F_{j,t} + \\epsilon_{i,t}$ where $F_{j,t}$ includes economic state variables\n",
    "\n",
    "2. **Business Cycle Classification Methods:**\n",
    "   1. **Official Dating Approaches:**\n",
    "      1. NBER recession dating (U.S. focused, comprehensive but lagged)\n",
    "      2. CEPR cycle dating (European equivalent of NBER)\n",
    "      3. OECD CLI (Composite Leading Indicators for international coverage)\n",
    "   2. **Quantitative Classification Methods:**\n",
    "      1. GDP growth rate thresholds (e.g., below 0% for recession)\n",
    "      2. Markov-switching models (Hamilton regime switching approach)\n",
    "      3. Statistical filtering techniques (Hodrick-Prescott, Kalman)\n",
    "   3. **Market-Based Indicators:**\n",
    "      1. Yield curve slopes (term spread as recession predictor)\n",
    "      2. Credit spreads (widening indicates economic stress)\n",
    "      3. Volatility regime clustering (GARCH-based regime identification)\n",
    "\n",
    "3. **Commodity-Specific Economic Sensitivity:**\n",
    "   1. Pro-cyclical commodities (industrial metals, energy) with positive GDP growth correlation\n",
    "   2. Counter-cyclical commodities (gold, sometimes agricultural) as inflation/recession hedges\n",
    "   3. Supply constraint effects creating idiosyncratic cycle responses\n",
    "   4. Monetary policy transmission effects on commodity pricing\n",
    "\n",
    "4. **Academic Sources:**\n",
    "   1. Fama, E. F., & French, K. R. (1989). Business conditions and expected returns on stocks and bonds. Journal of Financial Economics, 25(1), 23-49\n",
    "   2. Hamilton, J. D. (1989). A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica, 57(2), 357-384\n",
    "   3. Gorton, G., & Rouwenhorst, K. G. (2006). Facts and fantasies about commodity futures. Financial Analysts Journal, 62(2), 47-68\n",
    "   4. Erb, C. B., & Harvey, C. R. (2006). The strategic and tactical value of commodity futures. Financial Analysts Journal, 62(2), 69-97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cd1a4",
   "metadata": {},
   "source": [
    "### 5.2 Implementation Methodology\n",
    "\n",
    "1. **Constructing the Economic Regime Database:**\n",
    "   1. Download NBER recession indicators from FRED database (US) or comparable source\n",
    "   2. Create a binary recession indicator series aligned with asset return dates\n",
    "   3. Consider additional granularity beyond binary classification (expansion, late-cycle, early recession, late recession)\n",
    "   4. Validate official dates with real-time indicators to address publication lag issues\n",
    "   5. Extend regime classification internationally for global commodity analysis\n",
    "\n",
    "2. **Regime-Conditional Performance Analysis Steps:**\n",
    "   1. Segment return series by economic regime classification\n",
    "   2. Calculate conditional performance metrics for each regime\n",
    "   3. Test statistical significance of performance differentials across regimes\n",
    "   4. Analyze strategy correlation matrices conditional on economic state\n",
    "   5. Examine volatility persistence characteristics across different regimes\n",
    "\n",
    "3. **Volatility Targeting Strategy Adaptation to Economic Regimes:**\n",
    "   1. Evaluate efficacy of volatility scaling across different economic environments\n",
    "   2. Compare volatility clustering behavior in expansions versus recessions\n",
    "   3. Test regime-dependent parameter optimization (adaptive target volatility)\n",
    "   4. Assess timing delay impact on economic transition periods\n",
    "   5. Implement regime-switching volatility models to capture structural changes\n",
    "\n",
    "4. **Factor vs. Commodity Analysis Across Economic States:**\n",
    "   1. Compare defensive characteristics of volatility-managed portfolios across asset classes\n",
    "   2. Analyze beta to market conditional on economic regimes\n",
    "   3. Examine drawdown protection efficacy during recession periods\n",
    "   4. Test inflation sensitivity differences between managed and unmanaged portfolios\n",
    "   5. Evaluate diversification benefits across economic environments\n",
    "\n",
    "5. **Implementation Challenges and Solutions:**\n",
    "   1. Challenge: Regime classification is only known with certainty in hindsight\n",
    "      - Solution: Implement probabilistic regime models with leading indicators\n",
    "   2. Challenge: Limited recession observations in sample period\n",
    "      - Solution: Synthetic stress testing and bootstrap resampling methods\n",
    "   3. Challenge: Heterogeneity of recessions (financial, pandemic, supply-shock)\n",
    "      - Solution: Sub-classify recession types for more granular analysis\n",
    "   4. Challenge: International economic cycle asynchronicity\n",
    "      - Solution: Country-specific regimes or global factor extraction\n",
    "   5. Challenge: Structural breaks in economic relationships\n",
    "      - Solution: Time-varying parameter models or rolling estimation windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of economic cycle analysis functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas_datareader.data import DataReader\n",
    "from datetime import datetime\n",
    "\n",
    "def download_nber_recession_data(start_date='1990-01-01', end_date=None):\n",
    "    \"\"\"\n",
    "    Download NBER recession indicators from FRED\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str\n",
    "        End date in 'YYYY-MM-DD' format (defaults to current date)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with recession indicators (1=recession, 0=expansion)\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        # Download NBER recession indicator data from FRED\n",
    "        # Note: This requires internet connection and pandas_datareader\n",
    "        recession_data = DataReader('USREC', 'fred', start_date, end_date)\n",
    "        recession_data.columns = ['Recession']\n",
    "        return recession_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading recession data: {e}\")\n",
    "        print(\"Creating mock recession data for demonstration purposes...\")\n",
    "        \n",
    "        # Create mock recession data for demonstration\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "        mock_data = pd.DataFrame(index=date_range, columns=['Recession'])\n",
    "        \n",
    "        # Define some mock recessions (2001, 2008-2009, 2020)\n",
    "        mock_data['Recession'] = 0\n",
    "        mock_data.loc['2001-03-01':'2001-11-01', 'Recession'] = 1  # 2001 recession\n",
    "        mock_data.loc['2007-12-01':'2009-06-01', 'Recession'] = 1  # Global Financial Crisis\n",
    "        mock_data.loc['2020-02-01':'2020-04-01', 'Recession'] = 1  # COVID-19 initial shock\n",
    "        \n",
    "        return mock_data\n",
    "\n",
    "def align_recession_data_with_returns(returns, recession_data):\n",
    "    \"\"\"\n",
    "    Align recession indicators with return data dates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pandas.DataFrame\n",
    "        DataFrame with asset returns\n",
    "    recession_data : pandas.DataFrame\n",
    "        DataFrame with recession indicators\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Returns DataFrame with added recession indicator\n",
    "    \"\"\"\n",
    "    # Ensure recession data has daily frequency if returns are daily\n",
    "    if recession_data.index.freq != returns.index.freq:\n",
    "        recession_daily = recession_data.asfreq('D', method='ffill')\n",
    "    else:\n",
    "        recession_daily = recession_data\n",
    "    \n",
    "    # Align with returns index\n",
    "    aligned_recession = recession_daily.reindex(returns.index, method='ffill')\n",
    "    \n",
    "    # Add to returns DataFrame\n",
    "    returns_with_regime = returns.copy()\n",
    "    returns_with_regime['Recession'] = aligned_recession['Recession']\n",
    "    \n",
    "    return returns_with_regime\n",
    "\n",
    "def analyze_regime_performance(returns, regime_column='Recession'):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics conditional on economic regime\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pandas.DataFrame\n",
    "        Returns with regime indicator column\n",
    "    regime_column : str\n",
    "        Name of the column containing regime indicator\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict of pandas.DataFrame\n",
    "        Performance metrics by regime\n",
    "    \"\"\"\n",
    "    # Get unique regimes\n",
    "    regimes = returns[regime_column].unique()\n",
    "    \n",
    "    performance_by_regime = {}\n",
    "    asset_columns = [col for col in returns.columns if col != regime_column]\n",
    "    \n",
    "    for regime in regimes:\n",
    "        # Filter returns for current regime\n",
    "        regime_returns = returns[returns[regime_column] == regime][asset_columns]\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        metrics = calculate_performance_metrics(regime_returns)\n",
    "        performance_by_regime[int(regime)] = metrics\n",
    "    \n",
    "    return performance_by_regime\n",
    "\n",
    "def plot_regime_performance_comparison(performance_by_regime, metric='Annualized Return', figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Plot performance comparison across economic regimes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    performance_by_regime : dict\n",
    "        Dict of DataFrames with performance metrics by regime\n",
    "    metric : str\n",
    "        Performance metric to compare\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        Figure with regime comparison plot\n",
    "    \"\"\"\n",
    "    # Extract the specified metric from each regime's performance data\n",
    "    regime_names = {0: 'Expansion', 1: 'Recession'}\n",
    "    metric_by_regime = {regime_names[regime]: df.loc[metric] for regime, df in performance_by_regime.items()}\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = pd.DataFrame(metric_by_regime)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plot_data.plot(kind='bar', ax=ax)\n",
    "    \n",
    "    ax.set_title(f'{metric} by Economic Regime')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (would be implemented with actual data)\n",
    "# recession_data = download_nber_recession_data('2000-01-01')\n",
    "# returns_with_regime = align_recession_data_with_returns(all_strategy_returns, recession_data)\n",
    "# regime_performance = analyze_regime_performance(returns_with_regime)\n",
    "# fig = plot_regime_performance_comparison(regime_performance, metric='Sharpe Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b2674",
   "metadata": {},
   "source": [
    "## 6. Transaction Costs and Leverage Constraints Analysis\n",
    "\n",
    "### 6.1 Theoretical Framework for Implementation Constraints\n",
    "\n",
    "1. **Trading Cost Models in Asset Management:**\n",
    "   1. Definition: The frictional expenses incurred when executing trades in financial markets\n",
    "   2. Core components: Explicit costs (commissions, fees) and implicit costs (bid-ask spreads, market impact)\n",
    "   3. Mathematical formulation: $TC = \\sum_{t=1}^{T} \\sum_{i=1}^{N} |w_{i,t} - w_{i,t-1}^+| \\cdot c_i$ where $w_{i,t-1}^+$ is the weight before rebalancing\n",
    "   4. Impact on performance: Net return = Gross return - Transaction costs\n",
    "\n",
    "2. **Transaction Cost Components for Different Asset Classes:**\n",
    "   1. **Factor Implementation Costs:**\n",
    "      1. Explicit costs: Lower for large-cap factors, higher for small-cap factors\n",
    "      2. Implicit costs: Significant market impact for momentum due to high turnover\n",
    "      3. Capacity constraints: Lower liquidity in certain factor legs (e.g., small-cap value)\n",
    "   2. **Commodity Implementation Costs:**\n",
    "      1. Futures roll costs: Contango/backwardation effects on roll yield\n",
    "      2. Exchange fees and broker commissions (relatively standardized)\n",
    "      3. Bid-ask spreads: Wider for less liquid commodities (e.g., certain agricultural contracts)\n",
    "      4. Commodity-specific market impact models based on time of day, seasonality\n",
    "\n",
    "3. **Leverage Constraints and Their Implications:**\n",
    "   1. Regulatory constraints (e.g., UCITS 2x gross exposure limit for retail funds)\n",
    "   2. Risk management policy constraints (internal limits based on VaR budgets)\n",
    "   3. Practical funding constraints (margin requirements for futures)\n",
    "   4. Behavioral/institutional constraints (client tolerance for leverage)\n",
    "\n",
    "4. **Academic Sources:**\n",
    "   1. Frazzini, A., Israel, R., & Moskowitz, T.J. (2018). Trading costs. Working Paper\n",
    "   2. Novy-Marx, R., & Velikov, M. (2016). A taxonomy of anomalies and their trading costs. Review of Financial Studies, 29(1), 104-147\n",
    "   3. de Roon, F.A., Nijman, T.E., & Veld, C. (2000). Hedging pressure effects in futures markets. Journal of Finance, 55(3), 1437-1456\n",
    "   4. DeMiguel, V., Martin-Utrera, A., & Nogales, F.J. (2015). Parameter uncertainty in multiperiod portfolio optimization with transaction costs. Journal of Financial and Quantitative Analysis, 50(6), 1443-1471"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33134b79",
   "metadata": {},
   "source": [
    "### 6.2 Implementation Methodology\n",
    "\n",
    "1. **Transaction Cost Modeling Framework:**\n",
    "   1. Define cost parameters specific to each asset class (factors vs. commodities)\n",
    "   2. Implement proportional cost models (basis points of traded value)\n",
    "   3. Calculate turnover statistics across rebalancing periods\n",
    "   4. Measure net-of-cost performance for different strategy variants\n",
    "   5. Conduct sensitivity analysis across multiple cost scenarios (10, 30, 50 bps)\n",
    "\n",
    "2. **Optimizing Trading Efficiency:**\n",
    "   1. Implement rebalancing thresholds to reduce unnecessary trades (e.g., only rebalance when allocation differs by >5%)\n",
    "   2. Evaluate optimal rebalancing frequency (daily vs. weekly vs. monthly)\n",
    "   3. Apply portfolio optimization with transaction cost penalty terms\n",
    "   4. Implement trading rule smoothing to reduce turnover spikes\n",
    "   5. Analyze trade sizing to manage market impact costs\n",
    "\n",
    "3. **Leverage Constraint Implementation Methods:**\n",
    "   1. Hard capping approach (strictly enforce maximum leverage at each rebalancing)\n",
    "   2. Soft constraint approach (penalty function in optimization framework)\n",
    "   3. Dynamic leverage adjustment based on market volatility regimes\n",
    "   4. Scenario analysis of different leverage caps (1x, 2x, 3x, unconstrained)\n",
    "   5. Evaluate performance impact of leverage constraints across asset classes\n",
    "\n",
    "4. **Volatility Targeting Strategy Adaptations:**\n",
    "   1. Introduce trading buffers around target volatility to reduce turnover\n",
    "   2. Implement partial rebalancing techniques to smooth position changes\n",
    "   3. Explore adaptive volatility targeting parameters based on cost sensitivity\n",
    "   4. Analyze trade-off between targeting precision and implementation costs\n",
    "   5. Test cost-aware volatility scaling algorithms\n",
    "\n",
    "5. **Implementation Challenges and Solutions:**\n",
    "   1. Challenge: High turnover during volatility regime shifts\n",
    "      - Solution: Implement gradual position adjustments over multiple days\n",
    "   2. Challenge: Commodity-specific roll costs affecting performance\n",
    "      - Solution: Optimize roll timing and method (Goldman roll, enhanced roll)\n",
    "   3. Challenge: Heterogeneous trading costs across asset universe\n",
    "      - Solution: Asset-specific cost models and liquidity-aware position sizing\n",
    "   4. Challenge: Leverage spikes during sudden volatility drops\n",
    "      - Solution: Implement volatility forecasting to anticipate regime changes\n",
    "   5. Challenge: Performance attribution between alpha and implementation effects\n",
    "      - Solution: Decompose returns into strategy, timing, and cost components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae48daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of transaction cost and leverage constraint analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_turnover(positions_df):\n",
    "    \"\"\"\n",
    "    Calculate turnover from position changes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    positions_df : pandas.DataFrame\n",
    "        DataFrame with positions over time\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with turnover statistics\n",
    "    \"\"\"\n",
    "    # Calculate position changes\n",
    "    position_changes = positions_df.diff().abs()\n",
    "    \n",
    "    # Calculate turnover statistics\n",
    "    daily_turnover = position_changes.sum(axis=1)\n",
    "    asset_turnover = position_changes.sum(axis=0)\n",
    "    \n",
    "    # Annualize turnover (assuming daily data)\n",
    "    ann_turnover = daily_turnover.mean() * 252\n",
    "    \n",
    "    return {\n",
    "        'daily_turnover': daily_turnover,\n",
    "        'asset_turnover': asset_turnover,\n",
    "        'annualized_turnover': ann_turnover\n",
    "    }\n",
    "\n",
    "def apply_transaction_costs(returns_df, positions_df, cost_bps=10):\n",
    "    \"\"\"\n",
    "    Apply transaction costs to returns based on position changes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns_df : pandas.DataFrame\n",
    "        DataFrame with strategy returns\n",
    "    positions_df : pandas.DataFrame\n",
    "        DataFrame with positions over time\n",
    "    cost_bps : float or pandas.Series\n",
    "        Transaction cost in basis points (can be series for asset-specific costs)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Returns after transaction costs\n",
    "    \"\"\"\n",
    "    # Convert cost to decimal\n",
    "    cost_decimal = cost_bps / 10000\n",
    "    \n",
    "    # Calculate position changes\n",
    "    position_changes = positions_df.diff().abs()\n",
    "    \n",
    "    # Calculate costs for each day\n",
    "    if isinstance(cost_decimal, pd.Series):\n",
    "        # Asset-specific costs\n",
    "        costs = position_changes.multiply(cost_decimal, axis=1)\n",
    "    else:\n",
    "        # Uniform cost\n",
    "        costs = position_changes * cost_decimal\n",
    "    \n",
    "    # Sum costs across assets for each day\n",
    "    daily_costs = costs.sum(axis=1)\n",
    "    \n",
    "    # Apply costs to returns (align indices)\n",
    "    aligned_costs = daily_costs.reindex(returns_df.index).fillna(0)\n",
    "    net_returns = returns_df - aligned_costs\n",
    "    \n",
    "    return net_returns\n",
    "\n",
    "def analyze_cost_sensitivity(returns_df, positions_df, cost_scenarios=[10, 30, 50]):\n",
    "    \"\"\"\n",
    "    Analyze strategy performance under different cost scenarios\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns_df : pandas.DataFrame\n",
    "        DataFrame with strategy returns\n",
    "    positions_df : pandas.DataFrame\n",
    "        DataFrame with positions over time\n",
    "    cost_scenarios : list\n",
    "        List of transaction cost scenarios in basis points\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with performance metrics for each cost scenario\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate performance for each cost scenario\n",
    "    for cost_bps in cost_scenarios:\n",
    "        # Apply costs\n",
    "        net_returns = apply_transaction_costs(returns_df, positions_df, cost_bps)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        metrics = calculate_performance_metrics(net_returns)\n",
    "        \n",
    "        results[f'Cost_{cost_bps}bps'] = metrics\n",
    "    \n",
    "    return results\n",
    "\n",
    "def implement_rebalancing_threshold(positions_df, current_positions, volatility, target_vol, \n",
    "                                   threshold_pct=5.0, max_leverage=3.0):\n",
    "    \"\"\"\n",
    "    Implement threshold-based rebalancing to reduce turnover\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    positions_df : pandas.DataFrame\n",
    "        DataFrame with historical positions\n",
    "    current_positions : pandas.Series\n",
    "        Current positions before rebalancing\n",
    "    volatility : pandas.Series\n",
    "        Current volatility estimates\n",
    "    target_vol : float\n",
    "        Target volatility level\n",
    "    threshold_pct : float\n",
    "        Rebalancing threshold as percentage of position\n",
    "    max_leverage : float\n",
    "        Maximum allowed leverage\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        New positions after threshold-based rebalancing\n",
    "    \"\"\"\n",
    "    # Calculate target positions\n",
    "    target_positions = (target_vol / volatility).clip(upper=max_leverage)\n",
    "    \n",
    "    # Calculate percentage difference between current and target\n",
    "    pct_diff = abs((target_positions - current_positions) / current_positions) * 100\n",
    "    \n",
    "    # Only rebalance positions that exceed threshold\n",
    "    rebalance_mask = pct_diff > threshold_pct\n",
    "    \n",
    "    # Create new positions\n",
    "    new_positions = current_positions.copy()\n",
    "    new_positions[rebalance_mask] = target_positions[rebalance_mask]\n",
    "    \n",
    "    return new_positions\n",
    "\n",
    "def plot_cost_sensitivity(cost_results, metric='Sharpe Ratio', figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Plot the impact of transaction costs on performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cost_results : dict\n",
    "        Dictionary with performance metrics for each cost scenario\n",
    "    metric : str\n",
    "        Performance metric to plot\n",
    "    figsize : tuple\n",
    "        Figure size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        Figure with cost sensitivity plot\n",
    "    \"\"\"\n",
    "    # Extract cost values from keys\n",
    "    costs = [int(k.split('_')[1].replace('bps', '')) for k in cost_results.keys()]\n",
    "    \n",
    "    # Extract metric values for each strategy and cost\n",
    "    strategies = cost_results[list(cost_results.keys())[0]].columns\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = pd.DataFrame(index=costs, columns=strategies)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for cost_key, metrics in cost_results.items():\n",
    "            cost = int(cost_key.split('_')[1].replace('bps', ''))\n",
    "            plot_data.loc[cost, strategy] = metrics.loc[metric, strategy]\n",
    "    \n",
    "    # Sort by cost\n",
    "    plot_data = plot_data.sort_index()\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        ax.plot(plot_data.index, plot_data[strategy], marker='o', label=strategy)\n",
    "    \n",
    "    ax.set_title(f'Impact of Transaction Costs on {metric}')\n",
    "    ax.set_xlabel('Transaction Costs (bps)')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (would be implemented with actual data)\n",
    "# turnover_stats = calculate_turnover(strategy_positions['EWMA_Vol'])\n",
    "# net_returns = apply_transaction_costs(strategy_returns['EWMA_Vol'], strategy_positions['EWMA_Vol'], 30)\n",
    "# cost_sensitivity = analyze_cost_sensitivity(strategy_returns['EWMA_Vol'], strategy_positions['EWMA_Vol'])\n",
    "# cost_plot = plot_cost_sensitivity(cost_sensitivity, metric='Sharpe Ratio')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
